{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 02. Generate Chunks\n",
                "\n",
                "## Why do we need chunking?\n",
                "Large Language Models (LLMs) have a limit on how much text they can read at once (called the **Context Window**). \n",
                "If we try to feed an entire book into an LLM, it will fail or forget the beginning.\n",
                "\n",
                "**Chunking** breaks our large documents into smaller, manageable pieces (e.g., 500 characters or tokens). This allows us to find the specific paragraph that answers a user's question."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Install Libraries\n",
                "We need `langchain`, a popular library for building LLM applications."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install langchain"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Load Raw Data\n",
                "We load the `raw_documents` table we created in the previous notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "spark = SparkSession.builder.getOrCreate()\n",
                "\n",
                "# Load the table into a DataFrame\n",
                "# Make sure we are using the correct schema\n",
                "spark.sql(\"USE rag_demo\")\n",
                "\n",
                "df_raw = spark.table(\"raw_documents\")\n",
                "display(df_raw)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Clean Text\n",
                "Before chunking, it's good practice to clean the text. We will remove HTML tags and extra spaces."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import col, udf\n",
                "from pyspark.sql.types import StringType\n",
                "import re\n",
                "\n",
                "# Define a plain Python function to clean text\n",
                "def clean_text(text):\n",
                "    if text is None:\n",
                "        return \"\"\n",
                "    # Remove HTML tags like <div> or <br>\n",
                "    text = re.sub(r'<[^>]+>', '', text)\n",
                "    # Replace multiple spaces with a single space\n",
                "    text = re.sub(r'\\s+', ' ', text).strip()\n",
                "    # Convert to lowercase for consistency\n",
                "    return text.lower()\n",
                "\n",
                "# Convert the Python function to a Spark UDF (User Defined Function)\n",
                "clean_text_udf = udf(clean_text, StringType())\n",
                "\n",
                "# Apply the cleaning function to our dataframe\n",
                "df_cleaned = df_raw.withColumn(\"cleaned_content\", clean_text_udf(col(\"raw_content\")))\n",
                "\n",
                "display(df_cleaned)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Split Text into Chunks\n",
                "We will use `RecursiveCharacterTextSplitter` from LangChain. This is smart enough to split text at logical points (like periods or newlines) so we don't cut sentences in half."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import pandas_udf, explode\n",
                "from pyspark.sql.types import ArrayType, StringType\n",
                "import pandas as pd\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "\n",
                "# Configuration\n",
                "CHUNK_SIZE = 500  # Number of characters per chunk\n",
                "CHUNK_OVERLAP = 50 # Overlap between chunks to maintain context\n",
                "\n",
                "# We use a Pandas UDF for better performance on large data\n",
                "@pandas_udf(ArrayType(StringType()))\n",
                "def chunk_text_udf(content_series: pd.Series) -> pd.Series:\n",
                "    # Initialize the splitter\n",
                "    splitter = RecursiveCharacterTextSplitter(\n",
                "        chunk_size=CHUNK_SIZE,\n",
                "        chunk_overlap=CHUNK_OVERLAP,\n",
                "        length_function=len,\n",
                "    )\n",
                "    \n",
                "    # Helper function to apply to each row\n",
                "    def split_content(content):\n",
                "        if not content:\n",
                "            return []\n",
                "        return splitter.split_text(content)\n",
                "    \n",
                "    # Apply to the whole series (column)\n",
                "    return content_series.apply(split_content)\n",
                "    \n",
                "    # Apply to the whole series (column)\n",
                "    return content_series.apply(split_content)\n",
                "\n",
                "# Apply the chunking UDF\n",
                "# This creates a list of chunks for each file\n",
                "df_chunked = df_cleaned.withColumn(\"chunks\", chunk_text_udf(col(\"cleaned_content\")))\n",
                "\n",
                "# 'Explode' the list so that each chunk gets its own row\n",
                "df_exploded = df_chunked.select(\n",
                "    col(\"source_file\"),\n",
                "    explode(col(\"chunks\")).alias(\"chunk_text\")\n",
                ")\n",
                "\n",
                "display(df_exploded)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Save to Silver Table\n",
                "We'll add a unique ID to each chunk and save it as `silver_chunks`. This is our processed data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import monotonically_increasing_id\n",
                "\n",
                "# Add a unique ID column\n",
                "df_final = df_exploded.withColumn(\"chunk_id\", monotonically_increasing_id())\n",
                "\n",
                "# Save to Delta table\n",
                "df_final.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_chunks\")\n",
                "\n",
                "print(\"Success! Saved chunks to 'silver_chunks' table.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}