{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 01. Data Ingestion\n",
                "\n",
                "## Introduction\n",
                "Welcome to the **Databricks RAG (Retrieval-Augmented Generation) Demo**!\n",
                "\n",
                "In this series of notebooks, we will build a complete AI system that can answer questions based on your own documents.\n",
                "\n",
                "### What is this notebook for?\n",
                "This first notebook is all about **getting data ready**. We will:\n",
                "1.  Create a database (Schema) to organize our tables.\n",
                "2.  Create some dummy text files (simulating uploaded PDFs or docs).\n",
                "3.  Read these files and save them into a **Delta Table**.\n",
                "\n",
                "### What is Delta Lake?\n",
                "Delta Lake is the storage layer we use. It's like a super-powered version of CSV or Parquet files that supports:\n",
                "- **ACID Transactions**: Keeps data safe and consistent.\n",
                "- **Versioning**: You can see history of changes.\n",
                "- **Speed**: Optimized for fast reading and writing."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Setup Database\n",
                "We need a place to store our tables. In Databricks, we use a **Schema** (also called a Database)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a new schema named 'rag_demo' if it doesn't exist yet\n",
                "spark.sql(\"CREATE SCHEMA IF NOT EXISTS rag_demo\")\n",
                "\n",
                "# Switch to this schema so all future tables are created here\n",
                "spark.sql(\"USE rag_demo\")\n",
                "\n",
                "# NOTE: We are using the default Hive Metastore (or local DB) which works on Community Edition."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Create Sample Data\n",
                "Since we don't have external files yet, we will use Python to create some simple text files directly in the Databricks File System (DBFS)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Define the folder where we will save our raw text files\n",
                "# /dbfs/FileStore/ is a special folder in Databricks that acts like a hard drive\n",
                "raw_data_path = \"/dbfs/FileStore/rag_data/\"\n",
                "\n",
                "# Create the directory if it doesn't exist\n",
                "os.makedirs(raw_data_path, exist_ok=True)\n",
                "\n",
                "# These are our sample documents. In a real project, these would be your PDFs or Docs.\n",
                "sample_docs = {\n",
                "    \"doc1.txt\": \"Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads.\",\n",
                "    \"doc2.txt\": \"Databricks is a unified data analytics platform for massive scale data engineering and data science.\",\n",
                "    \"doc3.txt\": \"Retrieval-Augmented Generation (RAG) combines an LLM with a retrieval system to provide accurate, up-to-date answers.\",\n",
                "    \"doc4.txt\": \"Apache Spark is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.\"\n",
                "}\n",
                "\n",
                "# Loop through the dictionary and write each file to the disk\n",
                "for filename, content in sample_docs.items():\n",
                "    file_path = os.path.join(raw_data_path, filename)\n",
                "    with open(file_path, \"w\") as f:\n",
                "        f.write(content)\n",
                "\n",
                "print(f\"Success! Created sample files in {raw_data_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Ingest Data into Delta\n",
                "Now we use **PySpark** to read those text files and save them into a structured table."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import input_file_name, current_timestamp\n",
                "\n",
                "# 1. Read the text files\n",
                "# 'wholetext' means we read the entire content of the file into one row\n",
                "df_raw = spark.read.format(\"text\") \\\n",
                "    .option(\"wholetext\", True) \\\n",
                "    .load(\"dbfs:/FileStore/rag_data/*.txt\")\n",
                "\n",
                "# 2. Add some useful metadata columns\n",
                "# input_file_name() tells us which file the row came from\n",
                "df_raw = df_raw.withColumnRenamed(\"value\", \"raw_content\") \\\n",
                "    .withColumn(\"source_file\", input_file_name()) \\\n",
                "    .withColumn(\"ingestion_time\", current_timestamp())\n",
                "\n",
                "# 3. Display the dataframe to check our work\n",
                "display(df_raw)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Save as Table\n",
                "Finally, we save this dataframe as a **Delta Table** named `raw_documents`. This is our \"Bronze\" layer (raw data)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Write the data to a table\n",
                "# mode(\"overwrite\") means if the table exists, replace it entirely\n",
                "df_raw.write.format(\"delta\").mode(\"overwrite\") \\\n",
                "    .saveAsTable(\"raw_documents\")\n",
                "\n",
                "print(\"Table 'raw_documents' created successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's verify using SQL\n",
                "display(spark.sql(\"SELECT * FROM raw_documents\"))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
