{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 01. Data Ingestion\n",
                "\n",
                "## Introduction\n",
                "Welcome to the **Databricks RAG (Retrieval-Augmented Generation) Demo**!\n",
                "\n",
                "In this series of notebooks, we will build a complete AI system that can answer questions based on your own documents.\n",
                "\n",
                "### What is this notebook for?\n",
                "This first notebook is all about **getting data ready**. We will:\n",
                "1.  Create a database (Schema) to organize our tables.\n",
                "2.  Create some dummy text data (simulating uploaded PDFs or docs).\n",
                "3.  Save this data into a **Delta Table**.\n",
                "\n",
                "### What is Delta Lake?\n",
                "Delta Lake is the storage layer we use. It's like a super-powered version of CSV or Parquet files that supports:\n",
                "- **ACID Transactions**: Keeps data safe and consistent.\n",
                "- **Versioning**: You can see history of changes.\n",
                "- **Speed**: Optimized for fast reading and writing."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Setup Database\n",
                "We need a place to store our tables. In Databricks, we use a **Schema** (also called a Database)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Initializing SparkSession...\")\n",
                "from pyspark.sql import SparkSession\n",
                "spark = SparkSession.builder.getOrCreate()\n",
                "\n",
                "# Create a new schema named 'rag_demo' if it doesn't exist yet\n",
                "spark.sql(\"CREATE SCHEMA IF NOT EXISTS rag_demo\")\n",
                "\n",
                "# Switch to this schema so all future tables are created here\n",
                "spark.sql(\"USE rag_demo\")\n",
                "\n",
                "# NOTE: We are using the default Hive Metastore (or local DB) which works on Community Edition."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Create Sample Data\n",
                "Since we are in a restricted environment where file system access might be limited, we will create the data directly in memory using Spark."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import current_timestamp, lit\n",
                "from pyspark.sql.types import StringType, StructType, StructField\n",
                "\n",
                "# Sample data simulating documents\n",
                "sample_docs = [\n",
                "    (\"doc1.txt\", \"Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads.\"),\n",
                "    (\"doc2.txt\", \"Databricks is a unified data analytics platform for massive scale data engineering and data science.\"),\n",
                "    (\"doc3.txt\", \"Retrieval-Augmented Generation (RAG) combines an LLM with a retrieval system to provide accurate, up-to-date answers.\"),\n",
                "    (\"doc4.txt\", \"Apache Spark is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.\")\n",
                "]\n",
                "\n",
                "# Define schema\n",
                "schema = StructType([\n",
                "    StructField(\"source_file\", StringType(), True),\n",
                "    StructField(\"raw_content\", StringType(), True)\n",
                "])\n",
                "\n",
                "# Create DataFrame directly\n",
                "df_raw = spark.createDataFrame(sample_docs, schema)\n",
                "\n",
                "# Add ingestion time\n",
                "df_raw = df_raw.withColumn(\"ingestion_time\", current_timestamp())\n",
                "\n",
                "display(df_raw)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Save as Table\n",
                "Finally, we save this dataframe as a **Delta Table** named `raw_documents`. This is our \"Bronze\" layer (raw data)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Write the data to a table\n",
                "# mode(\"overwrite\") means if the table exists, replace it entirely\n",
                "df_raw.write.format(\"delta\").mode(\"overwrite\") \\\n",
                "    .saveAsTable(\"raw_documents\")\n",
                "\n",
                "print(\"Table 'raw_documents' created successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's verify using SQL\n",
                "display(spark.sql(\"SELECT * FROM raw_documents\"))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}