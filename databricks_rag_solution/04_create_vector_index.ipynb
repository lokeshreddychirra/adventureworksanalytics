{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 04. Create Vector Index (FAISS)\n",
                "\n",
                "## What is a Vector Index?\n",
                "We have our embeddings (lists of numbers), but if we want to find the *closest* match to a user's question, we can't just check every single row one by one\u2014that would be too slow for millions of documents.\n",
                "\n",
                "A **Vector Index** is a special data structure that allows us to search through these numbers extremely fast.\n",
                "\n",
                "Since Databricks Free Edition doesn't have the managed \"Vector Search\" feature, we will build our own index using a library called **FAISS** (Facebook AI Similarity Search)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Install FAISS\n",
                "We install the CPU version of FAISS."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install faiss-cpu"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Load Embeddings\n",
                "We load our `gold_embeddings` table."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import faiss\n",
                "import numpy as np\n",
                "import os\n",
                "import pickle\n",
                "\n",
                "spark.sql(\"USE rag_demo\")\n",
                "\n",
                "# Load the table\n",
                "df_embeddings = spark.table(\"gold_embeddings\")\n",
                "\n",
                "# Convert to Pandas so we can work with it locally on the driver\n",
                "# NOTE: This is okay for small demos. For huge datasets, you wouldn't do this.\n",
                "pdf = df_embeddings.select(\"chunk_id\", \"embedding\").toPandas()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Build the Index\n",
                "We convert our list of embeddings into a format FAISS understands and build the index."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Convert the 'embedding' column to a numpy array of floats\n",
                "embeddings_list = pdf[\"embedding\"].tolist()\n",
                "embeddings_array = np.array(embeddings_list).astype(\"float32\")\n",
                "\n",
                "# 2. Get the dimension size (how many numbers in each list?)\n",
                "# For our model, this should be 384.\n",
                "d = embeddings_array.shape[1]\n",
                "\n",
                "# 3. Create the index\n",
                "# IndexFlatL2 measures the 'distance' between points. Closer = more similar.\n",
                "index = faiss.IndexFlatL2(d)\n",
                "\n",
                "# 4. Add our vectors to the index\n",
                "index.add(embeddings_array)\n",
                "\n",
                "print(f\"Success! Built index with {index.ntotal} vectors.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Save Index and Metadata\n",
                "FAISS only stores the vectors, not the text or IDs. We need to save:\n",
                "1.  The **FAISS Index** file.\n",
                "2.  A **Mapping** file that tells us \"Vector #5 corresponds to Chunk ID #102\"."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a dictionary mapping: FAISS ID -> Chunk ID\n",
                "id_mapping = {i: chunk_id for i, chunk_id in enumerate(pdf[\"chunk_id\"])}\n",
                "\n",
                "# Define paths\n",
                "local_tmp_dir = \"/tmp/rag_data_tmp/\"\n",
                "os.makedirs(local_tmp_dir, exist_ok=True)\n",
                "\n",
                "local_index_path = os.path.join(local_tmp_dir, \"faiss_index.bin\")\n",
                "local_mapping_path = os.path.join(local_tmp_dir, \"id_mapping.pickle\")\n",
                "\n",
                "dbfs_dir = \"dbfs:/FileStore/rag_data/\"\n",
                "dbutils.fs.mkdirs(dbfs_dir)\n",
                "dbfs_index_path = dbfs_dir + \"faiss_index.bin\"\n",
                "dbfs_mapping_path = dbfs_dir + \"id_mapping.pickle\"\n",
                "\n",
                "# Save locally first\n",
                "faiss.write_index(index, local_index_path)\n",
                "\n",
                "with open(local_mapping_path, \"wb\") as f:\n",
                "    pickle.dump(id_mapping, f)\n",
                "\n",
                "# Move to DBFS\n",
                "dbutils.fs.cp(\"file:\" + local_index_path, dbfs_index_path)\n",
                "dbutils.fs.cp(\"file:\" + local_mapping_path, dbfs_mapping_path)\n",
                "\n",
                "print(f\"Index saved to {dbfs_index_path}\")\n",
                "print(f\"Mapping saved to {dbfs_mapping_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Log to MLflow (Optional)\n",
                "MLflow is a tool for tracking machine learning experiments. We can log our index as an artifact so we can find it later."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import mlflow\n",
                "\n",
                "# Start an MLflow run\n",
                "with mlflow.start_run(run_name=\"faiss_index_creation\"):\n",
                "    # Log some stats\n",
                "    mlflow.log_param(\"num_vectors\", index.ntotal)\n",
                "    mlflow.log_param(\"embedding_dim\", d)\n",
                "    \n",
                "    # Log the actual files\n",
                "    mlflow.log_artifact(index_path, artifact_path=\"index\")\n",
                "    mlflow.log_artifact(mapping_path, artifact_path=\"index\")\n",
                "    \n",
                "    print(\"Logged artifacts to MLflow run.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}