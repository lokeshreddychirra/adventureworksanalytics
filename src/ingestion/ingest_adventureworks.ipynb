{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Incremental Ingestion from SQL Server to Bronze Layer\n",
                "\n",
                "This notebook ingests data from a SQL Server table into a Delta Lake managed table in Unity Catalog.\n",
                "It uses a watermark (ModifiedDate) to load only new or modified records."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# DBTITLE 1,Configuration Widgets\n",
                "dbutils.widgets.text(\"jdbc_host\", \"\", \"JDBC Host\")\n",
                "dbutils.widgets.text(\"jdbc_port\", \"1433\", \"JDBC Port\")\n",
                "dbutils.widgets.text(\"jdbc_database\", \"AdventureWorks\", \"JDBC Database\")\n",
                "dbutils.widgets.text(\"jdbc_user\", \"\", \"JDBC User\")\n",
                "dbutils.widgets.text(\"jdbc_password\", \"\", \"JDBC Password\") # In prod, use secrets!\n",
                "dbutils.widgets.text(\"source_schema\", \"Sales\", \"Source Schema\")\n",
                "dbutils.widgets.text(\"source_table\", \"SalesOrderHeader\", \"Source Table\")\n",
                "dbutils.widgets.text(\"target_catalog\", \"main\", \"Target Catalog\")\n",
                "dbutils.widgets.text(\"target_schema\", \"bronze\", \"Target Schema\")\n",
                "dbutils.widgets.text(\"watermark_column\", \"ModifiedDate\", \"Watermark Column\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# DBTITLE 1,Get Parameters\n",
                "jdbc_host = dbutils.widgets.get(\"jdbc_host\")\n",
                "jdbc_port = dbutils.widgets.get(\"jdbc_port\")\n",
                "jdbc_database = dbutils.widgets.get(\"jdbc_database\")\n",
                "jdbc_user = dbutils.widgets.get(\"jdbc_user\")\n",
                "jdbc_password = dbutils.widgets.get(\"jdbc_password\")\n",
                "source_schema = dbutils.widgets.get(\"source_schema\")\n",
                "source_table = dbutils.widgets.get(\"source_table\")\n",
                "target_catalog = dbutils.widgets.get(\"target_catalog\")\n",
                "target_schema = dbutils.widgets.get(\"target_schema\")\n",
                "watermark_column = dbutils.widgets.get(\"watermark_column\")\n",
                "\n",
                "full_source_table = f\"{source_schema}.{source_table}\"\n",
                "full_target_table = f\"{target_catalog}.{target_schema}.{source_table.lower()}\"\n",
                "\n",
                "print(f\"Ingesting from {full_source_table} to {full_target_table}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# DBTITLE 1,JDBC Connection Properties\n",
                "jdbc_url = f\"jdbc:sqlserver://{jdbc_host}:{jdbc_port};database={jdbc_database};encrypt=true;trustServerCertificate=true\"\n",
                "connection_properties = {\n",
                "    \"user\": jdbc_user,\n",
                "    \"password\": jdbc_password,\n",
                "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# DBTITLE 1,Determine Watermark\n",
                "from pyspark.sql.functions import max, col, lit\n",
                "\n",
                "# Check if target table exists\n",
                "table_exists = spark.catalog.tableExists(full_target_table)\n",
                "\n",
                "watermark_value = \"1900-01-01 00:00:00\"\n",
                "\n",
                "if table_exists:\n",
                "    try:\n",
                "        # Get the max watermark from the target table\n",
                "        max_watermark_row = spark.table(full_target_table).select(max(col(watermark_column))).collect()[0][0]\n",
                "        if max_watermark_row:\n",
                "            watermark_value = max_watermark_row\n",
                "    except Exception as e:\n",
                "        print(f\"Warning: Could not get watermark from target table. Defaulting to {watermark_value}. Error: {e}\")\n",
                "\n",
                "print(f\"Current Watermark: {watermark_value}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# DBTITLE 1,Read from Source (Incremental)\n",
                "# Construct the query with the watermark filter\n",
                "# Note: We use dbtable with a subquery or pushdown predicate. \n",
                "# For simplicity and better pushdown, we can use the 'query' option or 'dbtable' with WHERE clause if supported, \n",
                "# but standard JDBC read often takes a table or a subquery.\n",
                "# A robust way is to use 'dbtable' as a subquery.\n",
                "\n",
                "query = f\"(SELECT * FROM {full_source_table} WHERE {watermark_column} > '{watermark_value}') AS src\"\n",
                "\n",
                "print(f\"Reading with query: SELECT * FROM {full_source_table} WHERE {watermark_column} > '{watermark_value}'\")\n",
                "\n",
                "df_source = spark.read.jdbc(\n",
                "    url=jdbc_url,\n",
                "    table=query,\n",
                "    properties=connection_properties\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# DBTITLE 1,Write to Target (Append)\n",
                "if df_source.count() > 0:\n",
                "    print(f\"Found {df_source.count()} new records. Writing to {full_target_table}...\")\n",
                "    \n",
                "    # Write to Delta\n",
                "    # We use 'append' for Bronze. \n",
                "    # If schema evolution is needed, we can enable mergeSchema.\n",
                "    (df_source.write\n",
                "        .format(\"delta\")\n",
                "        .mode(\"append\")\n",
                "        .option(\"mergeSchema\", \"true\")\n",
                "        .saveAsTable(full_target_table)\n",
                "    )\n",
                "    print(\"Write complete.\")\n",
                "else:\n",
                "    print(\"No new records found.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}