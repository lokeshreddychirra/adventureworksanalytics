{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 05. RAG Retrieval and Agent\n",
                "\n",
                "## The Finale: Building the Agent\n",
                "This is where everything comes together. We will build a simple AI agent that:\n",
                "1.  Takes your question.\n",
                "2.  **Retrieves** relevant information from our FAISS index.\n",
                "3.  **Generates** a natural language answer using an LLM (Large Language Model)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Install Libraries\n",
                "We need `transformers` to run the LLM."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install faiss-cpu sentence-transformers transformers"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Load Resources\n",
                "We need to load everything we built in previous steps:\n",
                "- The FAISS Index\n",
                "- The ID Mapping\n",
                "- The Embedding Model (to convert your question into numbers)\n",
                "- The LLM (to write the answer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import faiss\n",
                "import pickle\n",
                "import numpy as np\n",
                "from sentence_transformers import SentenceTransformer\n",
                "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
                "\n",
                "# 1. Load Index and Mapping\n",
                "index_path = \"/dbfs/FileStore/rag_data/faiss_index.bin\"\n",
                "mapping_path = \"/dbfs/FileStore/rag_data/id_mapping.pickle\"\n",
                "\n",
                "index = faiss.read_index(index_path)\n",
                "\n",
                "with open(mapping_path, \"rb\") as f:\n",
                "    id_mapping = pickle.load(f)\n",
                "\n",
                "# 2. Load Embedding Model\n",
                "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
                "\n",
                "# 3. Load LLM (Flan-T5 Small)\n",
                "# We use 'google/flan-t5-small' because it fits in the memory of the Free Edition.\n",
                "# It's not the smartest model, but it proves the concept.\n",
                "llm_model_name = \"google/flan-t5-small\" \n",
                "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
                "model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)\n",
                "\n",
                "# Create a pipeline for text generation\n",
                "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
                "\n",
                "print(\"All systems go! Resources loaded.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Define Retrieval Function\n",
                "This function takes a user's question and finds the most relevant text chunks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def retrieve_context(query, k=3):\n",
                "    \"\"\"\n",
                "    Searches for the top 'k' chunks relevant to the query.\n",
                "    \"\"\"\n",
                "    # 1. Convert query to vector\n",
                "    query_vector = embed_model.encode([query]).astype(\"float32\")\n",
                "    \n",
                "    # 2. Search FAISS index\n",
                "    # distances: how close the match is\n",
                "    # indices: the internal ID of the match\n",
                "    distances, indices = index.search(query_vector, k)\n",
                "    \n",
                "    # 3. Get the real Chunk IDs\n",
                "    retrieved_ids = [id_mapping[i] for i in indices[0] if i != -1]\n",
                "    \n",
                "    if not retrieved_ids:\n",
                "        return []\n",
                "    \n",
                "    # 4. Fetch the actual text from our Delta table\n",
                "    # We use Spark SQL to get the text for these IDs\n",
                "    ids_str = \",\".join([str(id) for id in retrieved_ids])\n",
                "    \n",
                "    df_context = spark.sql(f\"\"\"\n",
                "        SELECT chunk_text, source_file \n",
                "        FROM rag_demo.gold_embeddings \n",
                "        WHERE chunk_id IN ({ids_str})\n",
                "    \"\"\")\n",
                "    \n",
                "    return df_context.collect()\n",
                "\n",
                "# Let's test it!\n",
                "print(\"Testing Retrieval...\")\n",
                "results = retrieve_context(\"What is Delta Lake?\")\n",
                "for row in results:\n",
                "    print(f\"Found in {row.source_file}: {row.chunk_text[:100]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Define the RAG Agent\n",
                "This function combines Retrieval + Generation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def rag_agent(query):\n",
                "    print(f\"User Query: {query}\")\n",
                "    print(\"Thinking... (Retrieving context)\")\n",
                "    \n",
                "    # 1. Retrieve Context\n",
                "    context_rows = retrieve_context(query)\n",
                "    \n",
                "    # Combine all retrieved text into one big string\n",
                "    context_text = \"\\n\\n\".join([row.chunk_text for row in context_rows])\n",
                "    \n",
                "    # 2. Create the Prompt\n",
                "    # We tell the LLM exactly what to do\n",
                "    prompt = f\"\"\"\n",
                "    Answer the question based on the context below. If the answer is not in the context, say \"I don't know\".\n",
                "    \n",
                "    Context:\n",
                "    {context_text}\n",
                "    \n",
                "    Question:\n",
                "    {query}\n",
                "    \n",
                "    Answer:\n",
                "    \"\"\"\n",
                "    \n",
                "    # 3. Generate Answer\n",
                "    response = generator(prompt)\n",
                "    return response[0]['generated_text']"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Ask Questions!\n",
                "Now you can ask your RAG agent questions about the documents you uploaded."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example 1\n",
                "answer1 = rag_agent(\"Explain Delta Lake architecture\")\n",
                "print(f\"\\nAgent Answer: {answer1}\\n\")\n",
                "print(\"-\" * 50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example 2\n",
                "answer2 = rag_agent(\"What is Databricks?\")\n",
                "print(f\"\\nAgent Answer: {answer2}\\n\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}